import os
from os.path import dirname, join
from dotenv import load_dotenv
from io import BytesIO
import socket
import traceback
import streamlit as st
import openai
import time
from datetime import datetime, timezone
import json
import time
import base64
import re
import copy
from functools import reduce
from mimetypes import guess_type
import httpx
from openai import AssistantEventHandler, AzureOpenAI, OpenAI
from azure.ai.inference import ChatCompletionsClient
from azure.core.credentials import AzureKeyCredential
from typing_extensions import override
from dataclasses import dataclass, field
from typing import Any, List, Tuple, Dict, Optional, Union
from openai.types.file_object import FileObject
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionMessage,
    ChatCompletionChunk
)
from openai.types.beta.threads import (
    TextContentBlock,
    ImageURLContentBlock,
    ImageFileContentBlock,
    ImageFile,
    ImageURL,
    Text,
    Annotation,
    FileCitationAnnotation,
    Run
)
from openai.types.responses import (
    Response,
    ResponseUsage,
    ResponseFunctionToolCall,
    ResponseCodeInterpreterToolCall
)
from azure.ai.inference.models._models import (
    CompletionsUsage
)
import concurrent.futures
import customTools
import serperTools
import internetAccess
import processPDF
from cosmos_nosql import CosmosDB
from keepalive import login_state_extender

ContentBlock = ImageFileContentBlock | ImageURLContentBlock | TextContentBlock | ResponseCodeInterpreterToolCall

def get_sub_claim_or_ip():
    """
    Azure App Serviceä¸Šã§Easy Authã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹å ´åˆã€
    Xâ€‘MSâ€‘CLIENTâ€‘PRINCIPALãƒ˜ãƒƒãƒ€ãƒ¼ã«ã¯èªè¨¼æƒ…å ±ï¼ˆBase64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸJSONï¼‰ãŒå«ã¾ã‚Œã¾ã™ã€‚
    ã“ã®é–¢æ•°ã¯ã€Googleã®OIDCã‚’å‰æã¨ã—ã¦ã€ãã®èªè¨¼æƒ…å ±ã‹ã‚‰subã‚¯ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚
    ã„ãšã‚Œã‹ã®æ®µéšã§å¤±æ•—ã—ãŸå ´åˆã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¿”ã—ã¾ã™ã€‚
    """
    headers = st.context.headers
    if not headers:
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚µãƒ¼ãƒãƒ¼ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—ã—ã¦è¿”ã™
        server_ip = socket.gethostbyname(socket.gethostname())
        return f"no_header[{server_ip}]", None, None

    try:
        email = headers.get("X-Ms-Client-Principal-Name")
        sub = headers.get("X-Ms-Client-Principal-Id")
        name = None
        # Xâ€‘MSâ€‘CLIENTâ€‘PRINCIPALãƒ˜ãƒƒãƒ€ãƒ¼ã®å–å¾—
        client_principal_encoded = headers.get("X-Ms-Client-Principal") or headers.get("X-MS-CLIENT-PRINCIPAL") 
        if client_principal_encoded:
            # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
            decoded_bytes = base64.b64decode(client_principal_encoded)
            # JSONãƒ‘ãƒ¼ã‚¹
            principal = json.loads(decoded_bytes.decode("utf-8"))
            print(principal)
            claims = principal.get("claims", {})
            print(claims)
            claims = {claim["typ"]: claim["val"] for claim in claims}
            print(claims)

            if "name" in claims:
                name = claims["name"]

        if sub:
            return sub, email, name

    except Exception as e:
        st.error(f"èªè¨¼æƒ…å ±å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # Xâ€‘Forwardedâ€‘Forã¾ãŸã¯REMOTE_ADDRãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—ã™ã‚‹
    ip = headers.get("X-Forwarded-For") or headers.get("REMOTE_ADDR")
    print(headers.to_dict())
    if ip:
        return ip, None, None
    else:
        # IPã‚¢ãƒ‰ãƒ¬ã‚¹ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã€ã‚µãƒ¼ãƒãƒ¼ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—ã—ã¦è¿”ã™
        server_ip = socket.gethostbyname(socket.gethostname())
        return f"no_client_ip[{server_ip}]", None, None

@dataclass
class GPTHallucinatedFunctionCall:
    tool_uses: List['HallucinatedToolCalls']
    def __post_init__(self):
        self.tool_uses = [HallucinatedToolCalls(**i) for i in self.tool_uses]

@dataclass
class HallucinatedToolCalls:
    recipient_name: str
    parameters: dict

dotenv_path = join(dirname(__file__), ".env.local")
load_dotenv(dotenv_path)

tools=[{"type": "code_interpreter"}, {"type": "file_search"}, {"type": "web_search_preview" }, {"type": "image_generation"}, customTools.time, serperTools.run, serperTools.results, serperTools.scholar, serperTools.news, serperTools.places, internetAccess.html, processPDF.pdf]

class StreamHandler(AssistantEventHandler):
    @override
    def __init__(self, client):
        super().__init__()
        self.client = client
        # è¦ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨æœ€çµ‚çš„ãªrunã‚’å¼•ãæ¸¡ã™ï¼ˆtool_callã”ã¨ã«æ–°ã—ã„å­ã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒç”Ÿã˜ã‚‹ï¼‰
        self.content = []
        self.final_run = None

    @override
    def on_event(self, event):
      if event.event == 'thread.run.requires_action':
          run_id = event.data.id
          self.handle_requires_action(event.data, run_id)

    @override
    def on_image_file_done(self, image_file: ImageFile) -> None:
        print("on_image_file_done ImageFile:", image_file)
        self.content.append(ImageFileContentBlock(type="image_file", image_file=image_file))
        st.image(get_file(image_file.file_id))

    @override
    def on_text_done(self, text: Text) -> None:
        print("on_text_done Text:", text)
        self.content.append(TextContentBlock(type="text", text=text))
        value, files = parse_annotations(text.value, text.annotations)
        put_buttons(files, "stream")

    @override
    def on_tool_call_created(self, tool_call: Any) -> None:
        print(f"\nassistant > tool_call_created > {tool_call.type}\n", flush=True)
        if tool_call.type != "function":
            st.toast(tool_call.type)
        print(tool_call, flush=True)

    @override
    def on_tool_call_delta(self, delta: Any, snapshot: Any) -> None:
        if delta.type == "code_interpreter":
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end="", flush=True)
            if delta.code_interpreter.outputs:
                print("\n\noutput >", flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == "logs":
                        print(f"\n{output.logs}", flush=True)

    def handle_requires_action(self, data, run_id):
        print(f"\nassistant > {data}\n", flush=True)
        tool_calls = data.required_action.submit_tool_outputs.tool_calls

        tool_outputs = handle_tool_calls(tool_calls)

        with self.client.beta.threads.runs.submit_tool_outputs_stream(
            thread_id=self.current_run.thread_id,
            run_id=self.current_run.id,
            tool_outputs=tool_outputs,
            event_handler=StreamHandler(self.client)
        ) as stream:
            st.write_stream(stream.text_deltas)
            stream.until_done()
        # AssistantEventHandlerã«çµ„ã¿è¾¼ã¿ã®ã‚¤ãƒ™ãƒ³ãƒˆæ©Ÿæ§‹ã«ã‚ˆã‚Šã€thread.run.completed, canceled, 
        # expired, failed, required_action, incompleteã®éš›ã«ã€__current_runãŒæ›´æ–°ã•ã‚Œã€
        # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£current_run()ã«ã‚ˆã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹
        self.final_run = stream.final_run or stream.current_run
        self.content += stream.content


# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã®å®šç¾©
@dataclass
class ChatMessage:
    role: str
    # contentã¯Assistant APIã®contentå®šç¾©ã‚’å€Ÿç”¨
    content: List[ContentBlock]
    files: List[str] = field(default_factory=list)
    metadata: dict = field(default_factory=dict)

# ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†ã‚¯ãƒ©ã‚¹
class ChatThread:
    def __init__(self, client):
        self.client = client
        self.messages = []
        self.thread_id = None

    def add_message(self, model, role, content, files=None, metadata={}):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ """
        if isinstance(content, str):
            content = [TextContentBlock(type="text", text=Text(value=content, annotations=[]))]

        self.messages.append(ChatMessage(role, content, files, metadata))

        # gpt-4oã®Assistant APIã§ç”»åƒèªè­˜ãŒå‡ºæ¥ãªã„å•é¡Œã¯ã¨ã‚Šã‚ãˆãšãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã¨ã™ã‚‹
        # Assistant APIã¯ImageURLContentBlockã‚’èªè­˜ã—ãªã„ï¼Ÿã™ã‚‹ã¯ãšã ãŒãƒ»ãƒ»

        # Assistant APIã‚’æœªä½¿ç”¨ã®æ®µéšã§ã¯thread_idã¯å­˜åœ¨ã—ãªã„ã€‚åˆã‚ã¦ä½¿ã†æ™‚ã«ä½œæˆã—ã¦éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç™»éŒ²ã™ã‚‹ã€‚
        # Assistant APIæ™‚ã«ã¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯è‡ªå‹•çš„ã«Threadã«è¨˜éŒ²ã•ã‚Œã‚‹ã€‚
        if self.thread_id and (model["api_mode"] != "assistant" or role != "assistant"):
            self.client.beta.threads.messages.create(
                thread_id = self.thread_id,
                role = role,
                content = self.content_to_content_param(content),
                attachments = files
            )

    def get_last_message(self):
        return self.messages[-1]

    def get_last_message_id(self):
        """ç¾åœ¨è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®idã‚’è¿”ã™"""
        return len(self.messages) - 1

    def get_messages_after(self, id):
        """æŒ‡å®šã•ã‚ŒãŸidã‚ˆã‚Šå¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        return self.messages[(id + 1):]

    def get_thread_id(self):
        """
        Assistant APIç”¨ã®thread_idã‚’è¿”ã™ã€‚åˆã‚ã¦Assistant APIã‚’ä½¿ã†ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§
        threadã‚’ä½œæˆã—ã€éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç™»éŒ²ã™ã‚‹
        """
        if self.thread_id:
            return self.thread_id

        thread = self.client.beta.threads.create(
            messages = [
                {
                    "role": msg.role,
                    # ToDo: 32ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä»¥ä¸Šæºœã¾ã£ã¦ã‹ã‚‰ã ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã€‚
                    "content": self.content_to_content_param(msg.content),
                    "attachments": msg.files
                }
                for msg in self.messages if msg.role == "user" or msg.role == "assistant"
            ]
        )
        self.thread_id = thread.id
        
        return self.thread_id

    @staticmethod
    def content_to_content_param(content: List[ContentBlock]) -> List[dict]:
        """
        ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå½¢å¼ã®contentã‚’ã€APIé€ä¿¡ç”¨ã«dictã«å¤‰æ›ã™ã‚‹
        """
        content_param = []
        for block in content:
            if block.type == "text":
                # ã“ã®ã‚ãŸã‚Šã€å¾®å¦™ã«ä¸€å¯¾ä¸€é–¢ä¿‚ã§ã¯ãªã„
                content_param.append({
                    "type": block.type,
                    "text": block.text.value
                })
            elif block.type == "image_file":
                content_param.append({
                    "type": block.type,
                    "image_file": {"file_id": block.image_file.file_id, "detail": block.image_file.detail},
                })
            elif block.type == "image_url":
                content_param.append({
                    "type": block.type,
                    "image_url": {"url": block.image_url.url, "detail": block.image_url.detail},
                })
            elif block.type == "code_interpreter_call":
                content_param.append({
                    "type": block.type,
                    "id": block.id,
                    "container_id": block.container_id,
                    "code": block.code
                })
            else:
                raise ValueError(f"æœªçŸ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ–ãƒ­ãƒƒã‚¯ã® type: {block.type}")
        return content_param

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹
class ConversationManager:
    def __init__(self, clients, assistants):
        self.client = clients["openai"]
        self.thread = ChatThread(self.client)
        self.assistants = assistants
        self.response_id = None
        self.response_last_message_id = -1
        self.code_interpreter_file_ids = []

    def add_message(self, model, role, content, files=None, metadata={}):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ChatThreadã«è¿½åŠ """
        self.thread.add_message(model, role, content, files, metadata)

    def get_completion_messages(self, model, text_only=False):
        """Completion APIç”¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¤‰æ›"""
        messages = []
        for msg in self.thread.messages:
            # Assistant APIç”¨ã®ImageFileContentBlock, Response APIã®ResponseCodeInterpreterToolCallã¯é™¤ã
            content = [cont for cont in msg.content if not isinstance(cont, (ImageFileContentBlock, ResponseCodeInterpreterToolCall))]

            # Visionã‚µãƒãƒ¼ãƒˆã®ç„¡ã„ãƒ¢ãƒ‡ãƒ«ã«Imageã‚’ä¸ãˆã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã®ã§é™¤ã
            if not model.get("support_vision", False):
                content = [cont for cont in content if isinstance(cont, TextContentBlock)]

            if text_only:
                # Deepseekãªã©ã€ãƒ†ã‚­ã‚¹ãƒˆã ã‘å¿…è¦ãªå ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹
                content = "\n".join([cont.text.value for cont in content if isinstance(cont, TextContentBlock)])
            else:
                # ãã†ã§ãªã„å ´åˆã¯classã‹ã‚‰dictã«å¤‰æ›ã™ã‚‹
                content = self.thread.content_to_content_param(content)

            messages.append({
                "role": "assistant" if msg.role == "assistant" else "system" if msg.role == "system" else "system" if msg.role == "developer" else "user",
                "content": content
            })
        return messages

    # Response APIã«ã¦ã€AIã‹ã‚‰å›ç­”ãŒã‚ã£ãŸéš›ã€response.idã‚’è¨˜éŒ²ã—ã€ãã®idãŒã©ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¾ã§ã«å¯¾å¿œã—ã¦ã„ã‚‹ã‹ã‚’è¨˜éŒ²ã™ã‚‹
    def set_response_id(self, response_id):
        self.response_id = response_id
        self.response_last_message_id = self.thread.get_last_message_id()

    # ä¸€æ—¦code_interpreterã«ä¸ãˆãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥é™ã‚‚åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    def add_code_interpreter_file_ids(self, file_ids):
        self.code_interpreter_file_ids += file_ids
        # uniq
        self.code_interpreter_file_ids = list(dict.fromkeys(self.code_interpreter_file_ids))
        return self.code_interpreter_file_ids

    def get_response_history(self, model, offset = 0):
        """
        Response APIç”¨ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¤‰æ›
        é€šå¸¸ã¯å‰å›å¿œç­”ã®æ¬¡ã‹ã‚‰ã€‚reasoning withoutå•é¡Œå¯¾å¿œç”¨ã«ã€offset=-2ã§ãã®å‰ã®1ã‚¿ãƒ¼ãƒ³å‰ã«é¡ã‚Œã‚‹ã‚ˆã†ã«
        """

        def is_file_for(what_for, file):
            for t in file["tools"]:
                if t["type"] == what_for:
                    return True
            return False

        messages = []
        for msg in self.thread.get_messages_after(self.response_last_message_id + offset):
            content = msg.content
            if msg.role == "assistant":
                # Assistant APIç”¨ã®ImageFileContentBlockã¯é™¤ãã€‚ResponseOutputMessageParamã«ã¯imageã‚’æ·»ä»˜ã§ããªã„ã€‚
                # éš£æ¥ã™ã‚‹output_textã®annotationã¨ã—ã¦æ·»ä»˜ã™ã‚‹æ–¹æ³•ãŒã‚ã‚Šå¾—ã‚‹ãŒæœªå®Ÿè£…
                content = [cont for cont in msg.content if not isinstance(cont, ImageFileContentBlock)]

            # Visionã‚µãƒãƒ¼ãƒˆã®ç„¡ã„ãƒ¢ãƒ‡ãƒ«ã«Imageã‚’ä¸ãˆã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã®ã§é™¤ã
            if not model.get("support_vision", False):
                content = [cont for cont in content if isinstance(cont, TextContentBlock)]

            # classã‹ã‚‰dictã«å¤‰æ›ã™ã‚‹
            content = self.thread.content_to_content_param(content)

            # ToDo: æœ¬å½“ã¯typeã‚„ä¸è¦ãƒ–ãƒ­ãƒƒã‚¯ã®é™¤å»ã¯ChatThreadå†…ã«éš è”½ã™ã¹ãã€‚

            # "type"ã‚’Response APIå‘ã‘ã«ä¿®æ­£
            inout = "output" if msg.role == "assistant" else "input"
            content = [
                {
                    "text": cont["text"],
                    "type": inout + "_text"
                } if cont["type"] == "text" else
                {
                    "image_url": cont["image_url"]["url"],
                    "type": "input_image"
                } if cont["type"] == "image_url" else
                {
                    "file_id": cont["image_file"]["file_id"],
                    "type": "input_image"
                } if cont["type"] == "image_file" else
                cont
            for cont in content]

            # filesã‚’input_fileã¨ã—ã¦é€£çµ
            # Response APIã§ã¯ã€Visionå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã§ã€pdfã‚’"input_file"ã¨ã—ã¦è³ªå•ã«ä»˜åŠ ã§ãã‚‹ã€‚
            # ãƒ†ã‚­ã‚¹ãƒˆåŠã³å„ãƒšãƒ¼ã‚¸ã®ç”»åƒãŒãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‰ã‚Œã‚‹ã€‚
            # file["tools"]ãŒtype == "file_search"ã‚’å«ã‚€å ´åˆã€ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’input_fileã¨ã—ã¦æ‰±ã†
            # æ­£ç¢ºã«ã¯ã€ã“ã‚Œã¯vectoræ¤œç´¢ã‚’ç”¨ã„ã‚‹ã„ã‚ã‚†ã‚‹"file_search"ã¨ã¯ç•°ãªã‚‹æ©Ÿèƒ½
            # type == "code_interpreter"ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯åˆ¥é€”code_interpreter toolã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«æ·»ä»˜ã•ã‚Œã‚‹
            if msg.files:
                content += [
                    {
                        "file_id": file["file_id"],
                        "type": "input_file"
                    }
                for file in msg.files if is_file_for("file_search", file)]

            messages.append({
                "role": "assistant" if msg.role == "assistant" else "system" if msg.role == "system" else "developer" if msg.role == "developer" else "user",
                "content": content
            })

            file_ids_for_code_interpreter = [
                file["file_id"]
                for file in msg.files if is_file_for("code_interpreter", file)
            ] if msg.files else []
            file_ids_for_code_interpreter = self.add_code_interpreter_file_ids(file_ids_for_code_interpreter)
        return messages, self.response_id, file_ids_for_code_interpreter

    def create_attachments(self, files, tool_for_files):
        """Assistant, Response APIç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        attachments = []
        for file in files:
            file.seek(0)
            response = self.client.files.create(
                file=file,
                # Response APIã®ãŸã‚ã«ã¯purpose="user_data"ãŒæœ›ã¾ã—ã„ãŒã€2025/5/11ç¾åœ¨æœªå¯¾å¿œ 'Invalid value for purpose.'
                # "assistants"ã®ã¾ã¾ã ã¨Response APIã§ã€'APIError: An error occurred while processing the request.'
                # çµå±€Response APIã®input_fileã¨ã—ã¦ã¯ä½¿ãˆãªã„ â†’ 2025/8æ™‚ç‚¹ã§ã¯"input_file"ã¨ã—ã¦ä½¿ãˆã¦ã„ã‚‹ã€‚
                purpose="assistants"
            )
            # Response APIã§ã¯"file_search"ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®input_fileã«ã€"code_interpreter"ã¯code_intepreter toolã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦æ·»ä»˜ã™ã‚‹
            attachments.append(
                    {
                        "file_id": response.id,
                        "tools": [{"type": tool_for_files}],
                    }
            )
        return attachments

    def create_ImageURLContentBlock(self, file, detail_level):
        mime_type = guess_type(file.name)[0]
        image_encoded = base64.b64encode(file.getvalue()).decode()
        image_url = f'data:{mime_type};base64,{image_encoded}'
        return ImageURLContentBlock(
            type="image_url",
            image_url=ImageURL(url=image_url, detail=detail_level)
        )

# gpt-4oãŒAssistant APIã§ç”»åƒã‚’èªè­˜ã—ãªã„ã®ã§ã€ImageFileãªã‚‰ã¨æ€ã£ã¦åŠ ãˆãŸãŒã€ã©ã†ã‚„ã‚‰ãƒ¢ãƒ‡ãƒ«ã®æ–¹ã®å•é¡Œã‚‰ã—ã„
#    def create_ImageFileContentBlock(self, file, detail_level):
#        response = self.client.files.create(
#            file=file,
#            purpose="vision"
#            # "vision"ã‚’æŒ‡å®šã™ã‚‹ã¨ã€"purpose contains an invalid purpose vision"ã¨è¨€ã‚ã‚Œã¦ã—ã¾ã†ã€‚
#            # Azureã®APIãŒè¿½ã„ã¤ã„ã¦ã„ãªã„å¯èƒ½æ€§ã‚ã‚Šã€‚"assistant"ãªã‚‰å—ã‘ä»˜ã‘ã‚‹ãŒã€gpt-4oã¯è‡ªåˆ†ã«ã¯ç”»åƒèªè­˜èƒ½åŠ›ãŒç„¡ã„ã¨è¨€ã†
#        )
#        return ImageFileContentBlock(
#            type="image_file",
#            image_file=ImageFile(file_id=response.id, detail=detail_level)
#        )

def convert_parsed_response_to_assistant_messages(outputs: List[Any]) -> Tuple[List[ContentBlock], List[Dict[str, Any]]]:
    """
    Transform a Response API parsed_response.output list into a list of Assistant API style content blocks.

    """

    blocks: List[ContentBlock] = []
    metadata: List[Dict[str, Any]] = []

    def make_text_block(text: str, annotations: List[Any]) -> TextContentBlock:
        annotations = [
            FileCitationAnnotation(
                type="file_citation",
                text=ann.filename,
                start_index=ann.start_index,
                end_index=ann.end_index,
                file_citation={"file_id": f"{ann.file_id}|{ann.container_id}"}
            ) if ann.type == "container_file_citation" else ann
            for ann in annotations
        ]
        return TextContentBlock(type="text", text=Text(value=text, annotations=annotations))

    def make_image_url_block(url: str) -> ImageURLContentBlock:
        return ImageURLContentBlock(type="image_url", image_url=ImageURL(url=url, detail="auto"))

    def make_image_file_block(file_id: str, container_id: str|None = None) -> ImageFileContentBlock:
        return ImageFileContentBlock(type="image_file", image_file=ImageFile(file_id=f"{file_id}|{container_id}" if container_id else file_id, detail="auto"))

    def extract_annotations(text_value: str, raw_annotations: List[Any]) -> Tuple[List[ContentBlock], List[Dict[str, Any]]]:
        """
        Extract image-related annotations from a output_text of Response API response object and transform into Assistant API like image content block.
        Returns list of image/text blocks and metadata.
        """
        indexed_annotations = [ann for ann in raw_annotations if hasattr(ann, "start_index")]
        other_annotations = [ann for ann in raw_annotations if not hasattr(ann, "start_index")]
        pending_indexed =[]
        blocks = []
        metadata = []
        offset = 0
        for ann in sorted(indexed_annotations, key=lambda ann: ann.start_index):
            atype = getattr(ann, "type", None)
            start_index = int(getattr(ann, "start_index", 0)) - offset
            end_index = int(getattr(ann, "end_index", 0)) - offset

            # Clip to [0, len(text_value)]
            start_index = max(0, start_index)
            end_index = max(0, min(end_index, len(text_value)))

            pre_text = text_value[:start_index]
            ann_text = text_value[start_index:end_index]
            post_text = text_value[end_index:]

            if atype == "container_file_citation":
                container_id = getattr(ann, "container_id", None)
                file_id = getattr(ann, "file_id", None)
                filename = getattr(ann, "filename", "")
                if filename.lower().endswith(('.png', '.jpg', ".jpeg", ".gif", ".webp")):
                    if pending_indexed or pre_text:
                        blocks.append(make_text_block(pre_text, pending_indexed))
                        pending_indexed = []
                    blocks.append(make_image_file_block(file_id, container_id))
                    metadata.append({"filename": filename, "text":ann_text, "raw": ann})
                    offset += end_index
                    text_value = post_text
                    continue

            elif atype == "url_citation":
                url = getattr(ann, "url", None)
                title = getattr(ann, "title", None)
                if re.match(r'^data:', url):
                    if pending_indexed or pre_text:
                        blocks.append(make_text_block(pre_text, pending_indexed))
                        pending_indexed = []
                    blocks.append(make_image_url_block(url))
                    metadata.append({"title": title, "text":ann_text, "raw": ann})
                    offset += end_index
                    text_value = post_text
                    continue
            else:
                # ignore other annotation types
                pass

            ann_copy = copy.copy(ann)
            ann_copy.start_index = start_index
            ann_copy.end_index = end_index
            pending_indexed.append(ann_copy)

        blocks.append(make_text_block(text_value, pending_indexed + other_annotations))

        return blocks, metadata

    # Process outputs list in order
    for out_item in outputs:
        typ = getattr(out_item, "type", None)
        if typ == "message":
            content_items = getattr(out_item, "content", None) or []
            for content_item in content_items:
                ctype = getattr(content_item, "type", None)
                if ctype == "output_text":
                    text_value = getattr(content_item, "text", "") or ""
                    raw_annotations = getattr(content_item, "annotations", None) or []
                    eblocks, emetadata = extract_annotations(text_value, raw_annotations)
                    blocks += eblocks
                    metadata += emetadata

        elif typ == "code_interpreter_call":
            blocks.append(out_item)
            metadata.append({})

        elif typ == "image_generation_call":
            blocks.append(make_image_url_block("data:image/png;base64," + getattr(out_item, "result", "")))
            metadata.append({})

    return blocks, metadata

def pretty_print(messages: List[ChatMessage]) -> None:
    i = -1
    m = None
    for i0, m0 in enumerate(messages):
#        print("role:", m.role)
#        print("content:", m.content)
        if m0.role == "developer":
            continue
        if i != -1:
            with st.chat_message("assistant" if m.role == "assistant" else "user"):
                pretty_print_message(i, m)
        i = i0
        m = m0

    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿token_summaryã‚’è¡¨ç¤º
    if i != -1:
        with st.chat_message("assistant" if m.role == "assistant" else "user"):
            pretty_print_message(i, m, with_token_summary=True)


def pretty_print_message(key, message, with_token_summary=False):
    for j, cont in enumerate(message.content):
        if isinstance(cont, ImageFileContentBlock) and message.role == "assistant":
            # è‡ªåˆ†ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹ã®ã§ã€asssistantã®å ´åˆã®ã¿è¡¨ç¤º
            st.image(get_file(cont.image_file.file_id))
        if isinstance(cont, ImageURLContentBlock):
            st.image(cont.image_url.url)
        if isinstance(cont, TextContentBlock):
            value, files = parse_annotations(cont.text.value, cont.text.annotations)
            st.markdown(value, unsafe_allow_html=True)
            put_buttons(files, f"hist{key}-{j}")

    for j, cont in enumerate(message.content):
        if isinstance(cont, ResponseCodeInterpreterToolCall):
            container_id = getattr(cont, "container_id", None)
            outputs_attr = getattr(cont, "outputs", None) or []
            key_index = 1
            for out in outputs_attr:
                ctype = getattr(out, "type", None)
                if ctype == "image":
                    url = getattr(out, "url", None)
                    if url and url.startswith("data:"):
                        header, b64 = url.split(",", 1)
                        # MIMEã‚¿ã‚¤ãƒ—ã‚’å–å¾—ï¼ˆä¾‹: image/pngï¼‰
                        mime = header.split(":")[1].split(";")[0] if ":" in header else "application/octet-stream"
                        data_bytes = base64.b64decode(b64)

                        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã¨ MIME ã‚’æŒ‡å®šï¼‰
                        st.download_button(
                            label="ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=data_bytes,
                            file_name="image.png",
                            mime=mime,
                            key=f"download_buttun_{key}_{j}_{key_index}"
                        )

                if ctype == "logs" and st.session_state.show_code_and_logs:
                    with st.expander("code_interpreter logs"):
                        st.code(out.logs)

                key_index += 1

            code = getattr(cont, "code", None) or ""
            if code and st.session_state.show_code_and_logs:
                with st.expander("code_interpreter code"):
                    st.code(code)

#    print(message)
#    print(with_token_summary)
    if "file_search_results" in message.metadata:
        put_quotations(message.content, message.metadata["file_search_results"])
    if with_token_summary and "token_usage" in message.metadata:
        st.markdown(format_token_summary(message.metadata["token_usage"]))

def put_buttons(files, key=None) -> None:
    for i, file in enumerate(files):
        if key:
            key=f"{key}-{i}"
        else:
            key = None
        if file["type"] in ("file_path", "file_citation") :
            # Assistant APIã§ã¯"file_path"ã ã‘ã§è¶³ã‚ŠãŸæ¨¡æ§˜
            st.download_button(
                f"{file['index']}: {file['filename']} : ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                get_file(file["file_id"]),
                file_name=file["filename"],
                key=key
            )

def put_quotations(content, file_search_results):
    citations = {}
    for cont in content:
        if isinstance(cont, TextContentBlock):
            for annotation in cont.text.annotations:
                if annotation.type == "file_citation" and (match := re.search(r'(\d+):(\d+)', annotation.text)):
                    i = int(match[2])
                    if i not in citations:
                        citations[i] = annotation.text
    for i, text in citations.items():
        result = file_search_results[i]
        with st.expander(f"{text}: {result.file_name}"):
            st.write(f"""
~~~
score: {result.score}
~~~
{result.content[0].text}
""")

def get_file(file_id: str) -> bytes:
    key = f"content_{file_id}"
    if key in st.session_state.fileCache:
        return st.session_state.fileCache[key]

    client = st.session_state.clients["openaiv1"]
    if m := re.match(r'^([^|]*)\|([^|]*)$', file_id):
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚³ãƒ³ãƒ†ãƒŠã«ã‚ã‚‹å ´åˆ
        file_id = m.group(1)
        container_id = m.group(2)
        retrieve_file = client.containers.files.content.retrieve(file_id=file_id, container_id=container_id)
    else:
        retrieve_file = client.files.with_raw_response.content(file_id)
    content: bytes = retrieve_file.content
    st.session_state.fileCache[key] = content
    return content

def get_file_info(file_id: str) -> bytes:
    key = f"info_{file_id}"
    if key in st.session_state.fileCache:
        return st.session_state.fileCache[key]

    client = st.session_state.clients["openaiv1"]
    if m := re.match(r'^([^|]*)\|([^|]*)$', file_id):
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚³ãƒ³ãƒ†ãƒŠã«ã‚ã‚‹å ´åˆ
        file_id = m.group(1)
        container_id = m.group(2)
        res = client.containers.files.retrieve(file_id=file_id, container_id=container_id)
        retrieve_file = FileObject(object="file", id=res.id, bytes=res.bytes, created_at=res.created_at, filename=res.path, purpose="assistants", status="processed")
    else:
        retrieve_file = client.files.retrieve(file_id)
    st.session_state.fileCache[key] = retrieve_file
    return retrieve_file

def parse_annotations(value: str, annotations: List[Annotation]):
    files = []
#    print(value)
    print(f"annotations={annotations}")
    for (
        index,
        annotation,
    ) in enumerate(annotations):
        # FilePathAnnotation
        if annotation.type == "file_path":
            files.append(
                {
                    "type": annotation.type,
                    "file_id": annotation.file_path.file_id,
                    "filename": annotation.text.split("/")[-1],
                    "text": annotation.text,
                    "index": index
                }
            )
        elif annotation.type == "file_citation":
            if '|' in annotation.file_citation.file_id:
                # Response APIã®ContainerFileCitationç”±æ¥ã®å ´åˆ
                filename = annotation.text
            else:
                filename = get_file_info(annotation.file_citation.file_id).filename

            files.append(
                {
                    "type": annotation.type,
                    "file_id": annotation.file_citation.file_id,
                    "filename": filename,
                    "text": annotation.text,
                    "index": index
                }
            )
    value = re.sub(r'\[([^\]]*)\]\((sandbox:[^)]*)\)', r'ãƒœã‚¿ãƒ³\1', value)
    return value, files

def handle_tool_calls(tool_calls: List[Dict], mode = "assistant") -> List[Dict]:
    """
    ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
    """
    print(tool_calls)
    def add_output(outputs, tool_call_id, output, mode, fname = None):
        # Expected a string with maximum length 1048576, but got a string with length 7415317 instead
        # This model's maximum context length is 200000 tokens. However, your messages resulted in 416709 tokens
        if len(output.encode('utf-8')) > 200000:
            output = output[:200000]
        while len(output.encode('utf-8')) > 200000:
            output = output[:-10000]
        if mode == "assistant":
            # Assistant APIç”¨ã®tool_output
            outputs.append({
                "tool_call_id": tool_call_id,
                "output": output
            })
        elif mode == "response":
            # Response APIç”¨ã®tool_output
            outputs.append({
                "type": "function_call_output",
                "call_id": tool_call_id,
                "output": output
            })
        else:
            # Completion APIç”¨ã®tool_output
            outputs.append({
                "tool_call_id": tool_call_id,
                "role": "tool",
# 2025/4/17 API Referenceã§ã¯nameã¨ã„ã†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è¦æ±‚ã•ã‚Œã¦ã„ãªã„
#                "name": fname,
                "content": output,
            })

    tool_outputs = []

    for tool in tool_calls:
        # Response APIã§ã¯tool_callsã®å„è¦ç´ ã¯functionã‚’ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¨ã—ã¦æŒãŸãšã€ç›´ã«nameã‚„argumentsã‚’æŒã¤
        if hasattr(tool, "function"):
            function = tool.function
            call_id = tool.id
        else:
            function = tool
            call_id = tool.call_id
        fname = function.name
# ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆæ™‚ã«éƒ½åˆã‚ˆãç™ºç”Ÿã—ã¦ãã‚Œãªã„ã®ã§ï¼‰
#        function.name = "multi_tool_use.parallel"
#        function.arguments = '{"tool_uses": [{"recipient_name": "functions.get_google_results", "parameters": {"query": "OpenAI O1 processor"}}, {"recipient_name": "functions.get_google_results", "parameters": {"query": "OpenAI O1 chip"}}]}'
        fargs = json.loads(function.arguments)
        print(f"Function call: {fname}")
        print(f"Function arguments: {fargs}")

        if function.name == "multi_tool_use.parallel":
            # ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼šAIãŒä¸¦åˆ—å®Ÿè¡Œã‚’è¦æ±‚ã—ã¦ãã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
            # ä»•æ§˜å¤–ã®å‹•ä½œ(ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³)ã ã¨ã„ã†èª¬ã‚‚ã‚ã‚‹ã€‚å‹•ä½œæ¤œè¨¼æœªæ¸ˆã€‚
            # We need to deserialize the arguments
            caught_calls = GPTHallucinatedFunctionCall(**(json.loads(function.arguments)))
            tool_uses = caught_calls.tool_uses

            # ThreadPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
            with concurrent.futures.ThreadPoolExecutor() as executor:

                # å„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å®Ÿè¡Œç”¨ã‚¿ã‚¹ã‚¯ã«å¤‰æ›
                future_to_tool = {
                    executor.submit(
                        function_calling,
                        tool_use.recipient_name.rsplit('.', 1)[-1],
                        tool_use.parameters
                    ): {"id": call_id, "fname": tool_use.recipient_name.rsplit('.', 1)[-1]}
                    for tool_use in tool_uses
                }

                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’åé›†
                results = []
                for future in concurrent.futures.as_completed(future_to_tool):
                    tool_call_id = future_to_tool[future]["id"]
                    fname = future_to_tool[future]["fname"]
                    print(f"fname: {fname}")
                    print(f"tool_call_id: {tool_call_id}")
                    try:
                        results.append(future.result())
                    except Exception as e:
                        print(f"Error: {str(e)}")
                        results.append(json.dumps(f"Error: {str(e)}"))

                print(results)
                # ä¸¦åˆ—ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«ã¤ã„ã¦ã€tool_call_idã¯ä¸€ã¤ã—ã‹ãªãã€jsonã‚’æ”¹è¡Œã§é€£çµã—ä¸€ã¤ã®
                # tool_outputã«ã¾ã¨ã‚ã¦è¿”ã™(jsonl?)ã€‚ã“ã®æƒ…å ±ã¯ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ä¼šè©±ã‚ˆã‚Šå¾—ãŸãŒæ­£ã—ã„ã‹åˆ†ã‹ã‚‰ãªã„ã€‚
                add_output(tool_outputs, tool_call_id, "\n".join(results), mode, fname)
        else:
              # é †æ¬¡ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—
              fresponse = function_calling(fname, fargs)
              add_output(tool_outputs, call_id, fresponse, mode, fname)

    print("tool_outputs:")
    print(tool_outputs)
    return tool_outputs

def function_calling(fname, fargs):
        print(f"fc fname: {fname}")
        print(f"fc fargs: {fargs}")
        if fname == "get_current_weather":
            fresponse = customTools.get_current_weather(
                location=fargs.get("location"),
            )
        elif fname == "get_current_datetime":
            st.toast("[datetime]", icon="ğŸ•’");
            fresponse = customTools.get_current_datetime(
                timezone=fargs.get("timezone")
            )
        elif fname == "get_google_serper":
            st.toast(f"[Google Serper] {fargs.get('query')}", icon="ğŸ”");
            fresponse = serperTools.get_google_serper(
                query=fargs.get("query")
            )
        elif fname == "get_google_results":
            st.toast(f"[Google detail] {fargs.get('query')}", icon="ğŸ”");
            fresponse = serperTools.get_google_results(
                query=fargs.get("query")
            )
        elif fname == "get_google_scholar":
            st.toast(f"[Google scholar] {fargs.get('query')}", icon="ğŸ“");
            fresponse = serperTools.get_google_scholar(
                query=fargs.get("query")
            )
        elif fname == "get_google_news":
            st.toast(f"[Google news] {fargs.get('query')}", icon="ğŸ“°");
            fresponse = serperTools.get_google_news(
                query=fargs.get("query")
            )
        elif fname == "get_google_places":
            st.toast(f"[Google places] {fargs.get('query')}", icon="ğŸ½ï¸");
            fresponse = serperTools.get_google_places(
                query=fargs.get("query"),
                country=fargs.get("country", "jp"),
                language=fargs.get("language", "ja")
            )
        elif fname == "parse_html_content":
            st.toast("[parse html content]", icon="ğŸ‘€");
            fresponse = internetAccess.parse_html_content(
                url=fargs.get("url"),
                query=fargs.get("query", "headings"),
                heading=fargs.get("heading", None)
            )
        elif fname == "extract_pdf_content":
            st.toast("[extract pdf content]", icon="ğŸ‘€");
            fresponse = json.dumps(processPDF.extract_pdf_content(
                pdf_url=fargs.get("pdf_url"),
                page_range=fargs.get("page_range", None),
                image_id=fargs.get("image_id", None)
            ))
        return fresponse

# APIå®Ÿè¡Œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
def execute_api(model, selected_tools, conversation, streaming_enabled, options = {}):

    print(model)
    thread = conversation.thread
    client = model["client"]

    if model["api_mode"] == "inference":
        # DeepSeekã‚„Phiå‘ã‘ã®Inference API
        # https://learn.microsoft.com/en-us/rest/api/aifoundry/modelinference/
        messages = conversation.get_completion_messages(model, text_only=True)
        print(messages)
        try:
            if model["streaming"] and streaming_enabled:
                response = client.complete({
                    "stream": True,
                    "messages": messages,
# ç©ºã®toolsã‚’ä¸ãˆãŸã ã‘ã§ã‚‚ä¸å®‰å®šã«ãªã‚‹?ã„ã‚„ã€toolsã‚’ä¸ãˆãªãã¦ã‚‚ã“ã®ã‚¨ãƒ©ãƒ¼ã¯å‡ºã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ã‚µãƒ¼ãƒãƒ¼å´ã®æ··é›‘çŠ¶æ³ã«ã‚ˆã‚‹ã®ã§ã¯ãªã„ã‹ï¼Ÿ DeepSeek APIã‚¨ãƒ©ãƒ¼: Operation returned an invalid status 'Too Many Requests' Content: Please check this guide to understand why this error code might have been returned
#                    "tools": [],
# ç¾æ™‚ç‚¹ã§function callingã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã€‚toolsã‚’æŒ‡å®šã™ã‚‹ã¨ã€åå¿œãŒæ­¢ã¾ã£ã¦ã—ã¾ã†ã€‚2025/2
# https://github.com/deepseek-ai/DeepSeek-R1/issues/9
#                    "tools": [customTools.time, serperTools.run, serperTools.results, serperTools.news, serperTools.places, internetAccess.html],
                    "model": model["model"],
                    "max_tokens": 4096
                })
                print(response)
                digester = completion_streaming_digester(response)
                full_response = st.write_stream(digester.generator)
                response = digester.response
                response_message = ChatCompletionMessage.model_validate(response["choices"][0])
                print(response)
                full_response = response_message.content
            else:
                response = client.complete({
                    "messages": messages,
                    "max_tokens": 4096
                })
                full_response = response.choices[0].message.content

            token_usage = get_token_usage(response, model)
            st.markdown(format_token_summary(token_usage))
            metadata = {"token_usage": token_usage}
            conversation.add_message(model, "assistant", full_response, None, metadata)
            return full_response, metadata

        except Exception as e:
            st.error(f"Azure AI Model Inference APIã‚¨ãƒ©ãƒ¼")
            raise

    elif model["api_mode"] == "assistant":
        thread_id = conversation.thread.get_thread_id()
        print(thread_id)

        args = {"thread_id": thread_id, "assistant_id": model["assistant_id"]} | options

        if model["support_tools"]: # selected_toolsãŒç©ºã®å ´åˆã‚‚assistantè¨­å®šã‚’ä¸Šæ›¸ã
            args["tools"] = selected_tools

        if model["streaming"] and streaming_enabled:
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Assistant APIå®Ÿè¡Œ
            args["event_handler"] = StreamHandler(client)
            try:
                print(args)
                with client.beta.threads.runs.stream(**args) as stream:
                    st.write_stream(stream.text_deltas)
                    stream.until_done()

                run = stream.final_run or stream.current_run
                content = stream.content
                print(content)
                print(run)
                file_search_results = get_file_search_results(thread_id, run.id)
                put_quotations(content, file_search_results)
                token_usage = get_token_usage(run, model)
                st.markdown(format_token_summary(token_usage))
                metadata = {"token_usage": token_usage, "file_search_results": file_search_results}
                conversation.add_message(model, "assistant", content, None, metadata)
                return content, metadata

            except Exception as e:
                st.error(f"Assistant(streaming) APIã‚¨ãƒ©ãƒ¼")
                raise

        else:
            # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆ Assistant APIã®å®Ÿè¡Œ
            try:
                # å®Ÿè¡Œé–‹å§‹
                run = client.beta.threads.runs.create(**args)

                # å®Ÿè¡Œå®Œäº†ã‚’å¾…æ©Ÿ
                while run.status not in ["completed", "failed"]:
                    print(run)
                    time.sleep(1)
                    run = client.beta.threads.runs.retrieve(
                        thread_id=thread.thread_id,
                        run_id=run.id
                    )
                    if run.status == "requires_action":
                        print(run.required_action)
                        messages = handle_tool_calls(run.required_action.submit_tool_outputs.tool_calls, "assistant")
                        run = client.beta.threads.runs.submit_tool_outputs(
                          thread_id=thread.thread_id,
                          run_id=run.id,
                          tool_outputs=messages
                        )

                if run.status == "failed":
                    raise Exception(f"å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {run.last_error}")

                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
                print(run)
                messages = client.beta.threads.messages.list(
                    thread_id=thread.thread_id,
                    limit=1
                )
                print(messages)
                content = messages.data[0].content
                pretty_print_message("assist_msg", messages.data[0])
                token_usage = get_token_usage(run, model)
                st.markdown(format_token_summary(token_usage))
                metadata = {"token_usage": token_usage}
                conversation.add_message(model, "assistant", content, None, metadata)
                return content, metadata

            except Exception as e:
                st.error(f"Assistant APIã‚¨ãƒ©ãƒ¼")
                raise

    elif model["api_mode"] == "response":
        # Response APIå®Ÿè¡Œå‡¦ç†
        # ======== 2025/5/6 ç¾æ™‚ç‚¹ã§ã‚‚ã€web_search_preview, image_url pointing to an internet addressç­‰ãŒå®Ÿè£…ã•ã‚Œã¦ã„ãªã„ =======
        # https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/responses?tabs=python-secure

        client = st.session_state.clients["openaiv1"]
        input, response_id, file_ids_for_code_interpreter = conversation.get_response_history(model)
        try:
            args = {"model": model["model"], "input": input, "include": ["code_interpreter_call.outputs"]} | options

            reasoning_effort = args.pop('reasoning_effort', None)
            if reasoning_effort:
                args['reasoning'] = {"effort": reasoning_effort}

            if response_id:
                args["previous_response_id"] = response_id

            if model["support_tools"] and selected_tools:
                args["tools"] = prepare_tools_for_response_api(selected_tools, file_ids_for_code_interpreter)

            contents = []
            annotation_metadata = []
            full_response = ""
            tool_call_count = 0
            while True:
                print(f"args: {args}")

                try:
                    if model["streaming"] and streaming_enabled:
                        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Response APIå®Ÿè¡Œ
                        with client.responses.stream(**args) as stream:
                            digester = response_streaming_digester(stream)
                            full_response += st.write_stream(digester.generator)
                            stream.until_done()
                        response = digester.response

                    else:
                        # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Response APIå®Ÿè¡Œ
                        response = client.responses.create(**args)
                        st.write(response.output_text)
                        full_response += response.output_text

                except Exception as e:
                    print(e)
                    # reasoning withoutå•é¡Œã«å¯¾ã™ã‚‹å†è©¦è¡Œå‡¦ç†ã€‚APIãŒæ”¹å–„ã•ã‚Œã‚Œã°ä¸è¦ã«ãªã‚‹ã¯ãš
                    if (m := re.search(r"'(rs_[0-9a-f]+)' of type 'reasoning' was provided without its required following item\.", str(e))) and args["previous_response_id"]:
                        print("===== BadRequestError: 'reasoning' was provided without its required following...")
                        print("===== This may be a bug in API side.")
                        print(f"===== Retrying after removing the invalid reasoning item.")
                        failed_reasoning_id = m.group(1)
                        # ä¸€ã¤å‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«é¡ã£ã¦å–ã‚Šå‡ºã™
                        input_after_prev_user_input, response_id, file_ids_for_code_interpreter = conversation.get_response_history(model, -2)
                        # ä¸€ã¤å‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
                        prev_in_and_out = [input_after_prev_user_input[0]]
                        # ä¸€ã¤å‰ã®å¿œç­”ã¯ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å–ã‚Šå‡ºã™
                        prev_response = client.responses.retrieve(args["previous_response_id"])
                        # å•é¡Œã®reasoning itemã‚’å–ã‚Šé™¤ãã€‚messageä»¥å¤–ã®itemã¯item_referenceã«ã™ã‚‹
                        prev_in_and_out += [
                            out if out.type == "message" else {"type": "item_reference", "id": out.id}
                            for out in prev_response.output
                            if out.id != failed_reasoning_id
                        ]
                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å‰ã«ã€ãã®å‰ã®1ã‚¿ãƒ¼ãƒ³åˆ†ã®ã‚„ã‚Šå–ã‚Šã‚’æŒ¿å…¥
                        args["input"] = prev_in_and_out + args["input"]
                        print(args["input"])
                        # previous_response_idã«ã¯å‰ã®å‰ã®idã‚’ã‚»ãƒƒãƒˆã™ã‚‹
                        args["previous_response_id"] = prev_response.previous_response_id
                        continue

                    raise

                print(response)

                eblocks, emetadata = convert_parsed_response_to_assistant_messages(response.output)
                contents += eblocks
                annotation_metadata += emetadata

                # streamingæ™‚ã«å¾—ã‚‰ã‚Œã‚‹ParsedResponseFunctionToolCallã‚’ResponseFunctionToolCallã«castã™ã‚‹
                # ä½™è¨ˆãªãƒ—ãƒ­ãƒ‘ãƒ†ã‚£parsed_argumentsãŒã‚ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ã®ã§
                tool_calls = [
                    ResponseFunctionToolCall(arguments=mes.arguments, call_id=mes.call_id, name=mes.name, type=mes.type, id=mes.id, status=mes.status)
                    for mes in response.output if mes.type == 'function_call'
                ]
                if tool_calls:
                    # args["input"] += tool_calls
                    args["input"] = handle_tool_calls(tool_calls, "response")
                    args["previous_response_id"] = response.id
                    tool_call_count += 1

                else:
                    break

                if tool_call_count > 20:
                    raise Exception(f"tool callã®é€£ç¶šå®Ÿè¡Œå›æ•°ãŒåˆ¶é™ã‚’è¶…ãˆã¾ã—ãŸã€‚å›æ•°: {tool_call_count}")

            token_usage = get_token_usage(response, model)
            st.markdown(format_token_summary(token_usage))
            metadata = {"token_usage": token_usage, "annotations_metadata": annotation_metadata}
            conversation.add_message(model, "assistant", contents, None, metadata)
            conversation.set_response_id(response.id)
            st.session_state.need_rerun = True
            return full_response, metadata

        except Exception as e:
            st.error(f"Response APIã‚¨ãƒ©ãƒ¼")
            raise

    else:
        # Completion APIå®Ÿè¡Œå‡¦ç†
        messages = conversation.get_completion_messages(model)
        try:
            args = {"model": model["model"], "messages": messages} | options

            if model["streaming"] and streaming_enabled:
                args["stream"] = True
                args["stream_options"] = {"include_usage": True}

            if model["support_tools"] and selected_tools:
                args["tools"] = selected_tools

            full_response = ""
            tool_call_count = 0
            while True:
                print(f"args: {args}")
                response = client.chat.completions.create(**args)
                print(response)

                if model["streaming"] and streaming_enabled:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Completion APIå®Ÿè¡Œ
                    digester = completion_streaming_digester(response)
                    full_response += st.write_stream(digester.generator)
                    response = digester.response
                    print(response)
                    response_message = ChatCompletionMessage.model_validate(response["choices"][0])

                else:
                    # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã®Completion APIå®Ÿè¡Œ
                    response_message = response.choices[0].message
                    if hasattr(response_message, "content") and response_message.content:
                        st.write(response_message.content)
                        full_response += response_message.content

                messages.append(response_message.model_dump())

                if hasattr(response_message, "tool_calls") and response_message.tool_calls:
                    messages += handle_tool_calls(response_message.tool_calls, "completion")
                    tool_call_count += 1

                else:
                    break

                if tool_call_count > 20:
                    raise Exception(f"tool callã®é€£ç¶šå®Ÿè¡Œå›æ•°ãŒåˆ¶é™ã‚’è¶…ãˆã¾ã—ãŸã€‚å›æ•°: {tool_call_count}")
                
            token_usage = get_token_usage(response, model)
            st.markdown(format_token_summary(token_usage))
            metadata = {"token_usage": token_usage}
            conversation.add_message(model, "assistant", full_response, None, metadata)
            return full_response, metadata

        except Exception as e:
            st.error(f"Completion APIã‚¨ãƒ©ãƒ¼")
            raise

# Response APIã®functionå®šç¾©ã¯ãã‚Œä»¥å‰ã¨ç•°ãªã‚Šã€"function"ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ä¸‹ã«ã‚ã£ãŸå®šç¾©ãŒã€rootã«ç§»å‹•ã—ã¦ã„ã‚‹ã®ã§å¤‰æ›ã™ã‚‹
def prepare_tools_for_response_api(tools, file_ids_for_code_interpreter):
    new_tools = []

    for t in tools:
        t_type = t.get("type")

        if t_type == "function":
            new_tools.append(t.get("function", {}) | {"type": "function"})

        elif t_type == "code_interpreter":
            new_tools.append({
                "type": "code_interpreter",
                # Azureãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯"files"ã®ã¯ãšãªã®ã ãŒã€"Unknown parameter: 'tools[0].container.files'."
                # ã¨ãªã£ã¦ã—ã¾ã†ã€‚"file_ids"ãªã‚‰ã°å‹•ä½œã™ã‚‹ã€‚2025/8
                "container": {"type": "auto", "file_ids": file_ids_for_code_interpreter}
#                "container": {"type": "auto", "files": file_ids_for_code_interpreter}
            })

        else:
            new_tools.append(t)

    return new_tools

def get_file_search_results(thread_id, run_id):
    client = st.session_state.clients["openai"]

    # Run Stepã‚’å–å¾—
    run_steps = client.beta.threads.runs.steps.list(
        thread_id=thread_id,
        run_id=run_id
    )

    # æœ€å¾Œã®Run Stepã‹ã‚‰File Searchã®å®Ÿè¡Œçµæœã‚’å«ã‚ã¦å–å¾—ã™ã‚‹
    run_step = client.beta.threads.runs.steps.retrieve(
        thread_id=thread_id,
        run_id=run_id,
        step_id=run_steps.data[-1].id,
        include=["step_details.tool_calls[*].file_search.results[*].content"]
    )
    if hasattr(run_step.step_details, "tool_calls"):
        file_search_tcs = [tc for tc in run_step.step_details.tool_calls if hasattr(tc, "file_search")]
        if file_search_tcs:
            return file_search_tcs[0].file_search.results
    print(run_step)
    return []

def get_token_usage(response, model):
    if isinstance(response, ChatCompletion) or isinstance(response, Run):
        response = response.model_dump()
    usage = hasattr(response, 'usage') and response.usage or response.get("usage", None)
    if usage:
        if isinstance(usage, CompletionsUsage):
            usage = {"completion_tokens": usage["completion_tokens"], "prompt_tokens": usage["prompt_tokens"], "total_tokens": usage["total_tokens"]}
        elif isinstance(usage, ResponseUsage):
            usage = {"completion_tokens": usage.output_tokens, "prompt_tokens": usage.input_tokens, "total_tokens": usage.total_tokens}
        usage["cost"] = (usage["prompt_tokens"] * model["pricing"]["in"] + usage["completion_tokens"] * model["pricing"]["out"]) / 1000000
        usage["pricing"] = model["pricing"]
        return usage
    else:
        return {}

def format_token_summary(usage):
    token_summary = ""
    if reduce(
        lambda a, c:c in usage and a,
        ["completion_tokens", "prompt_tokens", "total_tokens", "cost"], True):
        token_summary = f"tokens in:{usage['prompt_tokens']} out:{usage['completion_tokens']} total:{usage['total_tokens']}"
        token_summary += f" cost: US${usage['cost']}"
        token_summary = f"\n:violet-background[{token_summary}]"

    return token_summary 

class response_streaming_digester:
    def __init__(self, stream):
        self.stream = stream
        self.response = {}

    def generator(self):
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åŠã³tool_callã®æ–­ç‰‡ã‚’å—ã‘å–ã‚ŠãªãŒã‚‰UIã«åæ˜ ã—ã€æœ€çµ‚çš„ã«å®Œå…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¾©å…ƒã™ã‚‹
    # å‚è€ƒ: https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses
#        print(self.stream)
        for event in self.stream:
#            print(f"event.type={event.type} event={event}\n", end="")
            if event.type == "response.refusal.delta":
                print(event.delta, end="")
            elif event.type == "response.output_text.delta":
#                print(event.delta, end="")
                yield event.delta
            elif event.type == "response.error":
                print(event.error, end="")
            elif event.type == "response.completed":
                print("Stream completed")
                # print(event.response.output)

        self.response = self.stream.get_final_response()

class completion_streaming_digester:
    def __init__(self, stream):
        self.stream = stream
        self.response = {}

    def generator(self):
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åŠã³tool_callã®æ–­ç‰‡ã‚’å—ã‘å–ã‚ŠãªãŒã‚‰UIã«åæ˜ ã—ã€æœ€çµ‚çš„ã«å®Œå…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¾©å…ƒã™ã‚‹
        for chunk in self.stream:

            if isinstance(chunk, ChatCompletionChunk):
                # OpenAIã®Compltion APIã®å ´åˆ
                chunk_dict = chunk.model_dump()
            else:
                # deepseekãªã©
                chunk_dict = chunk

            for key, value in chunk_dict.items():
                if key == "choices":
                    if key not in self.response:
                        self.response[key] = {}
                    for choice in value:
                        for ckey, cvalue in choice.items():
                            ci = choice["index"]
                            if ci not in self.response[key]:
                                self.response[key][ci] = {"content": "", "tool_calls": {}}
                            if ckey == "delta":
                                for dkey, dvalue in cvalue.items():
                                    if dkey == "content" and dvalue:
                                        self.response[key][ci][dkey] += dvalue
                                        # UIã«å‡ºåŠ›
                                        yield dvalue
                                    elif dkey == "tool_calls" and dvalue:
                                        for tool_call in dvalue:
                                            for tkey, tvalue in tool_call.items():
                                                ti = tool_call["index"]
                                                if ti not in self.response[key][ci][dkey]:
                                                    self.response[key][ci][dkey][ti] = {"function": {"arguments":""}}
                                                if tkey == "function" and tvalue:
                                                    if "name" in tvalue and tvalue["name"]:
                                                        self.response[key][ci][dkey][ti][tkey]["name"] = tvalue["name"]
                                                    if "arguments" in tvalue and tvalue["arguments"]:
                                                        self.response[key][ci][dkey][ti][tkey]["arguments"] += tvalue["arguments"]
                                                elif tvalue:
                                                    self.response[key][ci][dkey][ti][tkey] = tvalue
                                    elif dvalue:
                                        self.response[key][ci][dkey] = dvalue
                            elif cvalue:
                                self.response[key][ci][ckey] = cvalue
                elif value:
                    self.response[key] = value
        # ä¸Šè¨˜ãƒãƒ¼ã‚¸ä½œæ¥­ã®éƒ½åˆä¸Šdictã§è¡¨ç¾ã•ã‚ŒãŸé…åˆ—ã‚’listã«å¤‰æ›ã™ã‚‹
        choices = []
#        print(self.response)
        for ci, cvalue in sorted(self.response["choices"].items(), key=lambda x:x[0]):
            tool_calls = []
            for ti, tvalue in sorted(cvalue["tool_calls"].items(), key=lambda x:x[0]):
                tool_calls.append(tvalue)
            cvalue["tool_calls"] = tool_calls
            choices.append(cvalue)
        self.response["choices"] = choices

def get_assistant(client, mode):
    # IF: https://platform.openai.com/docs/assistants/how-it-works/creating-assistants
    current_time = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S+00:00")

    if mode == "development":
        instructions=f"ã‚ãªãŸã¯æ±ç”¨çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è³ªå•ã«ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚ç¾åœ¨ã®æ—¥æ™‚ã¯ã€Œ{current_time}ã€ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã€æ™‚æ©Ÿã«ã‹ãªã£ãŸå›ç­”ã‚’å¿ƒãŒã‘ã¾ã™ã€‚ã‚ãªãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æœ€æ–°ã®æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
        tools=[{"type": "code_interpreter"}, customTools.time, serperTools.run, serperTools.results, serperTools.news, serperTools.places, internetAccess.html, processPDF.pdf]
    else:
        instructions=f"ã‚ãªãŸã¯æ±ç”¨çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è³ªå•ã«ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚ç¾åœ¨ã®æ—¥æ™‚ã¯ã€Œ{current_time}ã€ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã€æ™‚æ©Ÿã«ã‹ãªã£ãŸå›ç­”ã‚’å¿ƒãŒã‘ã¾ã™ã€‚ã‚ãªãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æœ€æ–°ã®æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
        tools=[{"type": "code_interpreter"}, customTools.time, serperTools.run, serperTools.results, internetAccess.html, processPDF.pdf]

    name=f"æ±ç”¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ({mode})"
    assistant = None
    assistants = client.beta.assistants.list(order='desc', limit="100")
    print(assistants)
    for i in assistants.data:
        if i.created_at < time.time() - 86400:
            client.beta.assistants.delete(assistant_id=i.id)
            time.sleep(.2)
        elif i.name == name:
            assistant = i

    if not assistant:
        assistant = client.beta.assistants.create(
            name=name,
            model="gpt-4o"
        )

    client.beta.assistants.update(
        assistant_id=assistant.id,
        instructions=instructions,
        tools=tools
    )

    return assistant.id

# åˆæœŸåŒ–
if "db" not in st.session_state:
    st.session_state.db = CosmosDB(
        os.getenv("COSMOS_NOSQL_HOST"),
        os.getenv("COSMOS_NOSQL_MASTER_KEY"),
        'ToDoList',
        'Items'
    )
if "clients" not in st.session_state:
    st.session_state.clients = {
        # 2025/8æ™‚ç‚¹ã§ã¯container fileã«éå¯¾å¿œ
        "openai": AzureOpenAI(
            azure_endpoint = os.getenv("ENDPOINT_URL"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version="2025-04-01-preview",
            default_headers={"x-ms-oai-image-generation-deployment": "gpt-image-1"},
            timeout=httpx.Timeout(1200.0, read=1200.0, write=30.0, connect=10.0, pool=60.0)
        ),
        # v1 preview
        # 2025/8æ™‚ç‚¹ã§ã¯Assistant APIã«éå¯¾å¿œ
        "openaiv1": OpenAI(
            base_url = os.getenv("ENDPOINT_URL").rstrip("/") + "/openai/v1/",
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            default_query={"api-version": "preview"},
            default_headers={"x-ms-oai-image-generation-deployment": "gpt-image-1"},
            timeout=httpx.Timeout(1199.0, read=1200.0, write=30.0, connect=10.0, pool=60.0)
        ),
        "deepseek": ChatCompletionsClient(
            endpoint=os.getenv("DEEPSEEK_ENDPOINT_URL"),
            credential=AzureKeyCredential(os.getenv("DEEPSEEK_AZURE_INFERENCE_CREDENTIAL"))
        ),
        "phi4": ChatCompletionsClient(
            endpoint=os.getenv("PHI_4_ENDPOINT_URL"),
            credential=AzureKeyCredential(os.getenv("PHI_4_AZURE_INFERENCE_CREDENTIAL"))
        )
    }

if "assistants" not in st.session_state:
    st.session_state.assistants = {
        "gpt-4o": get_assistant(st.session_state.clients["openai"], os.getenv("IASA_DEPLOYMENT_MODE", "development"))
    }

models = {
  "GPT-5.2-response": {
    "model": "gpt-5.2",
    "client": st.session_state.clients["openai"],
    "api_mode": "response",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": True,
    "default_reasoning_effort": "medium",
    "streaming": True,
    "pricing": {"in": 1.75, "cached": 0.175, "out":14} #https://azure.microsoft.com/en-us/blog/introducing-gpt-5-2-in-microsoft-foundry-the-new-standard-for-enterprise-ai/
  },
  "GPT-5.2-chat-response": {
    "model": "gpt-5.2-chat",
    "client": st.session_state.clients["openai"],
    "api_mode": "response",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": True,
    "default_reasoning_effort": "medium",
    "streaming": True,
    "pricing": {"in": 1.75, "cached": 0.175, "out":14}
  },
  "model-router-completion": {
    "model": "model-router",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 1.25, "cached": 0.125, "out":10} # ã“ã‚Œã¯GPT-5ã®å˜ä¾¡ã€‚å®Ÿéš›ã«ã¯åˆ©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®å˜ä¾¡ã§è«‹æ±‚ã•ã‚Œã‚‹
  },
  "GPT-5.1-response": {
    "model": "gpt-5.1",
    "client": st.session_state.clients["openai"],
    "api_mode": "response",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": True,
    "default_reasoning_effort": "medium",
    "streaming": True,
    "pricing": {"in": 1.25, "cached": 0.13, "out":10}
  },
  "GPT-5.1-chat-response": {
    "model": "gpt-5.1-chat",
    "client": st.session_state.clients["openai"],
    "api_mode": "response",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": True,
    "default_reasoning_effort": "medium",
    "streaming": True,
    "pricing": {"in": 1.25, "cached": 0.13, "out":10}
  },
  "GPT-5.1-codex-max-response": {
    "model": "gpt-5.1-codex-max",
    "client": st.session_state.clients["openai"],
    "api_mode": "response",
    "support_vision": True,
    "support_tools": True,
    "support_code_interpreter": False,
    "support_reasoning_effort": ["low", "medium", "high", "xhigh"],
    "default_reasoning_effort": "medium",
    "streaming": True,
    "pricing": {"in": 1.25, "cached": 0.13, "out":10}
  },
  "GPT-5-mini-response": {
    "model": "gpt-5-mini",
    "client": st.session_state.clients["openai"],
    "api_mode": "response",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": True,
    "default_reasoning_effort": "medium",
    "streaming": True,
    "pricing": {"in": 0.25, "cached": 0.025, "out":2} # Azureã§ã®priceãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‚ã“ã‚Œã¯ã€https://learn.microsoft.com/en-us/answers/questions/5521675/what-is-internal-microsoft-pricing-for-using-gpt-5
  },
  "GPT-5-response": {
    "model": "gpt-5",
    "client": st.session_state.clients["openai"],
    "api_mode": "response",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": True,
    "default_reasoning_effort": "medium",
    "streaming": True,
    "pricing": {"in": 1.25, "cached": 0.125, "out":10} # Azureã§ã®priceãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‚ã“ã‚Œã¯ã€https://learn.microsoft.com/en-us/answers/questions/5521675/what-is-internal-microsoft-pricing-for-using-gpt-5
  },
  "GPT-4.1-response": {
    "model": "gpt-4.1",
    "client": st.session_state.clients["openai"],
    "api_mode": "response",
    "support_vision": True,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 2, "out":8} # Azureã§ã®priceãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‚ã“ã‚Œã¯Open AIã®ã‚‚ã®ã€‚
  },
  "o1": {
    "model": "o1",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": ["low", "medium", "high"],
    "streaming": False,
    "pricing": {"in": 15, "out":60}
  },
  "GPT-4.1-completion": {
    "model": "gpt-4.1",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 2, "out":8} # Azureã§ã®priceãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‚ã“ã‚Œã¯Open AIã®ã‚‚ã®ã€‚
  },
  "o3": {
    "model": "o3",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": ["low", "medium", "high"],
    "streaming": True,
    "pricing": {"in": 10, "out":40} # Azureã§ã®priceãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‚ã“ã‚Œã¯Open AIã®ã‚‚ã®ã€‚
  },
  "o3-mini": {
    "model": "o3-mini",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": False,
    "support_tools": True,
    "support_reasoning_effort": ["low", "medium", "high"],
    "streaming": True,
    "pricing": {"in": 1.1, "out":4.4}
  },
  "o4-mini": {
    "model": "o4-mini",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "support_reasoning_effort": ["low", "medium", "high"],
    "streaming": True,
    "pricing": {"in": 1.1, "out":4.4} # Azureã§ã®priceãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‚ã“ã‚Œã¯Open AIã®ã‚‚ã®ã€‚
  },
  "GPT-4.5-completion": {
    "model": "GPT-4.5-preview",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 75, "out":150}
  },
  "GPT-4o": {
    "model": "GPT-4o",
    "client": st.session_state.clients["openai"],
    "api_mode": "assistant",
    "assistant_id": st.session_state.assistants["gpt-4o"],
    "support_vision": False,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 2.5, "out":10}
  },
#  "GPT-4o-nostream": {
#    "model": "GPT-4o",
#    "client": st.session_state.clients["openai"],
#    "api_mode": "assistant",
#    "assistant_id": st.session_state.assistants["gpt-4o"],
#    "support_vision": False,
#    "support_tools": True,
#    "streaming": False,
#    "pricing": {"in": 2.5, "out":10}
#  },
  "GPT-4o-completion": {
    "model": "GPT-4o",
    "client": st.session_state.clients["openai"],
    "api_mode": "completion",
    "support_vision": True,
    "support_tools": True,
    "streaming": True,
    "pricing": {"in": 2.5, "out":10}
  },
  "DeepSeek R1": {
    "model": "DeepSeek-R1",
    "client": st.session_state.clients["deepseek"],
    "api_mode": "inference",
    "streaming": True,
    "pricing": {"in": 0, "out":0}
  },
  "Microsoft Phi-4": {
    "model": "Phi-4",
    "client": st.session_state.clients["phi4"],
    "api_mode": "inference",
    "streaming": True,
    # ã“ã‚Œã¯EastUS2ã®ä¾¡æ ¼ã§ã‚ã‚Šã€æ­£ç¢ºã§ã¯ãªã„
    # https://techcommunity.microsoft.com/blog/machinelearningblog/affordable-innovation-unveiling-the-pricing-of-phi-3-slms-on-models-as-a-service/4156495
    "pricing": {"in": 0.125, "out": 0.5}
  }
}

if "fileCache" not in st.session_state:
    st.session_state.fileCache = {}
if "processing" not in st.session_state:
    st.session_state.processing = False
if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = 0
if 'switches' not in st.session_state:
    st.session_state.switches = {
        "code_interpreter": True,
        "file_search": True,
        "web_search_preview": True,
        "get_google_results": True,
        "parse_html_content": True,
        "extract_pdf_content": True
    }
if "need_rerun" not in st.session_state:
    st.session_state.need_rerun = False

# ãƒ¡ã‚¤ãƒ³UI
st.subheader("IASA Chat Interface")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    principal, email, name = get_sub_claim_or_ip()
#    if name:
#        st.text("Name: " + name)
    if email:
        st.text("User: " + email)
    else:
        st.text("Principal: " + principal)

    model = models[st.selectbox(
        "Model",
        models.keys()
    )]
    options = {}

    st.text("API Mode: " + model["api_mode"])
    st.text("Streaming: " + ("True" if model.get("streaming", False) else "False"))
    st.text("Support vision: " + ("True" if model.get("support_vision", False) else "False"))

    if model.get("support_reasoning_effort", False):
        if isinstance(model["support_reasoning_effort"], list):
            reasoning_effort_choices = model["support_reasoning_effort"]
        else:
            reasoning_effort_choices = ["minimal", "low", "medium", "high"]
        options["reasoning_effort"] = st.selectbox(
            "reasoning_effort",
            reasoning_effort_choices,
            index = ({c: i for i, c in enumerate(reasoning_effort_choices)})[model.get("default_reasoning_effort", "high")]
        )

    uploaded_files = None
    image_files = None
    if model["api_mode"] == "assistant" or model["api_mode"] == "response":
        uploaded_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            accept_multiple_files=True,
            key = f"file_uploader_{st.session_state.uploader_key}"
        )
        if model["api_mode"] in ["assistant", "response"]:
            tool_for_files = st.selectbox(
                "ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”¨é€”",
                ["file_search", "code_interpreter"]
            )
            st.session_state.switches[tool_for_files] = True
        else:
            tool_for_files = "file_search"

    if model.get("support_vision", False):
        image_files = st.file_uploader(
            "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«",
            accept_multiple_files=True,
            type = ["png", "jpeg", "jpg", "webp", "gif"],
            key = f"image_uploader_{st.session_state.uploader_key}"
        )
        detail_level = st.selectbox(
            "ç”»åƒã®è©³ç´°ãƒ¬ãƒ™ãƒ«",
            ["auto", "high", "low"]
        )

    if model["api_mode"] == "assistant":
        supported_tool_types = ["function", "code_interpreter", "file_search"]
    elif model["api_mode"] == "response":
        supported_tool_types = ["function", "code_interpreter", "image_generation"]
# web_search_previewã¯ç¾åœ¨å®Ÿè£…ã•ã‚Œã¦ãŠã‚‰ãšã€file_searchã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã«ã¯vector storeã®ç®¡ç†æ©Ÿèƒ½ãŒå¿…è¦
#        tools = [tool for tool in tools if tool.get("type", None) in ["function", "web_search_preview", "file_search"]
# web_search_previewãŒä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚Œã°ã€ã“ã‚Œã‚‰ã®ãƒ„ãƒ¼ãƒ«ã¯ä¸è¦ã«ãªã‚‹
        #            and (tool.get("type", None) != "function" or tool["function"]["name"] not in ["get_google_results", "parse_html_content", "extract_pdf_content"])
    else:
        # "completion", "inference"ã§ä½¿ãˆã‚‹ã®ã¯functionã ã‘
        supported_tool_types = ["function"]

    tools = [tool for tool in tools if tool.get("type", None) in supported_tool_types]

    if model.get("support_code_interpreter", True) == False:
        tools = [tool for tool in tools if tool.get("type", None) != "code_interpreter"]

    # è¡¨ç¤ºç”¨ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
    tool_names = [
        t.get("type") if t.get("type") != "function"
        else t["function"]["name"]
        for t in tools
    ]

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«toolé¸æŠç”¨ãƒˆã‚°ãƒ«ã‚¹ã‚¤ãƒƒãƒã‚’è¡¨ç¤º
    if model.get("support_tools", False):
        st.header("ãƒ„ãƒ¼ãƒ«é¸æŠ")
        switches = {
            name: st.toggle(
                name,
                value=st.session_state.switches[name] if name in st.session_state.switches else False,
                key=f"tool_switch_{name}"
            )
            for name in tool_names
        }
        st.session_state.switches = st.session_state.switches | switches

        # é¸æŠã«åˆã‚ã›ãƒ„ãƒ¼ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        selected_tools = [
            tool
            for i, tool in enumerate(tools)
            if st.session_state.switches[tool_names[i]]
        ]
    else:
        selected_tools = []

    st.header("è¡¨ç¤ºè¨­å®š")
    if model["streaming"] and not switches.get("image_generation", False):
        streaming_enabled = st.toggle(
            "streaming mode",
            value=True,
            key="streaming"
        )
    # image_generation toolã¯(ãƒ†ã‚­ã‚¹ãƒˆã®)streaming modeã«ã¯å¯¾å¿œã—ã¦ã„ãªã„
    if switches.get("image_generation", False):
        streaming_enabled = False

    if model["api_mode"] == "assistant" or model["api_mode"] == "response":
        show_code_and_logs = st.toggle(
            "show code and logs",
            value=False,
            key="show_code_and_logs"
        )

    login_state_extender(email)

if "conversation" not in st.session_state:
    st.session_state.conversation = ConversationManager(st.session_state.clients, st.session_state.assistants)
    # developerãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
    # Formatting re-enabled: å‚è€ƒ https://learn.microsoft.com/ja-jp/azure/ai-foundry/openai/how-to/reasoning?tabs=gpt-5%2Cpython-secure%2Cpy#markdown-output
    st.session_state.conversation.add_message(model, "developer", 'Formatting re-enabled - please enclose code blocks with appropriate markdown tags. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ãŒæ›–æ˜§ãªå ´åˆã¯ã€ã¾ãšç°¡æ½”ã«ä¸€æ¬¡å›ç­”ã‚’æç¤ºã—ã€å¿…è¦ã«å¿œã˜ã¦ã€è³ªå•ã®æ„å›³ã‚’æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã®è³ªå•ã‚„æ–¹å‘æ€§ã®ææ¡ˆã‚’ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãƒ„ãƒ¼ãƒ«ã®åˆ©ç”¨å›æ•°ãŒã‚ã‚‹ä¸€ã¤ã®å¿œç­”ã®ãŸã‚ã ã‘ã«7å›ã‚’è¶…ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯ã€ã¾ãšæœ€å¤§4å›ä»¥å†…ã§åˆç†çš„ãªå›ç­”ã‚’è©¦ã¿ã€ãã®ä¸Šã§ã•ã‚‰ãªã‚‹ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ã®è¨ˆç”»ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«èª¬æ˜ã—ã€å®Ÿè¡Œã®åŒæ„ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚code_interpreterã‚’ç”¨ã„ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æä¾›ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¿…ãšãƒ¦ãƒ¼ã‚¶ãƒ¼å¯è¦–ã®ãƒ„ãƒ¼ãƒ«ï¼ˆä¾‹ï¼špython_user_visibleï¼‰ã§ /mnt/data ã«ç›´æ¥æ›¸ãå‡ºã— ã€åŒä¸€å®Ÿè¡Œã§ stdout ã«ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’å‡ºåŠ›ã—ã¦ã¦ãã ã•ã„ã€‚', [])
conversation = st.session_state.conversation 

if content := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›"):
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        attachments = []
        if uploaded_files:
            st.toast("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            attachments = conversation.create_attachments(uploaded_files, tool_for_files)
            st.toast("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            st.session_state.uploader_key += 1 # é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢

        if image_files:
            st.toast("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            content = [TextContentBlock(type="text", text=Text(value=content, annotations=[]))]
            for file in image_files:
                # ImageURLãŒã€ImageFileã®ä¸¡æ–¹ä½œæˆã—ã¦ãŠãã€APIå®Ÿè¡Œæ™‚ã«ä¸è¦ãªæ–¹ã¯æ¨ã¦ã‚‹
                # ãŸã ã—ã€Threadæœªä½œæˆæ™‚ã«ã¯ImageURLã®ã¿ä½œæˆã¨ã—ã€Threadä½œæˆæ™‚ã«ImageURLã‹ã‚‰å¤‰æ›ç”Ÿæˆã™ã‚‹
                # ã¨ã„ã†è¨ˆç”»ã ã£ãŸãŒã€ã©ã†ã‚„ã‚‰gpt-4oã¯Assistant APIæ™‚ã«ã¯ç”»åƒã‚’èªè­˜ã§ããªã„æ¨¡æ§˜ï¼ˆæœ¬äººè«‡ï¼‰
                # ç¾çŠ¶ã€Assistant API, Visionä¸¡å¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ãŒä»–ã«ãªã„ã®ã§ã€ä¸€æ—¦ã‚ãã‚‰ã‚ã€ImageURLã®ã¿å¯¾å¿œã€‚
                content.append(conversation.create_ImageURLContentBlock(file,detail_level))
            st.toast("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            st.session_state.uploader_key += 1 # é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢
    
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
        conversation.add_message(model, "user", content, attachments)
        # å‡¦ç†ä¸­ã¸ç§»è¡Œ
        st.session_state.processing = True
        st.rerun()  # ã“ã“ã§ä¸€æ—¦å†æç”»(ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªã‚¢ãªã©ã«å¿…è¦)
    
# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
pretty_print(conversation.thread.messages)

if st.session_state.get("processing"):
    # å‡¦ç†ã™ã¹ããƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹

    # APIå®Ÿè¡Œ
    try:
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        with st.chat_message("assistant"):

            content, metadata = execute_api(
                model,
                selected_tools,
                conversation,
                streaming_enabled,
                options
                )
        
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å±¥æ­´ã«è¿½åŠ 
            st.session_state.processing = False
            st.session_state.db.log({
                "principal": principal,
                "email": email,
                "name": name,
                "model": model["model"],
                "token_usage": metadata["token_usage"]
            }, principal)

            # æœ€çµ‚çš„ãªå†…å®¹ã§æç”»ã—ãªãŠã™ã¹ãã‹ï¼Ÿå½“é¢ä¸è¦ã¨åˆ¤æ–­ã€‚
            # placeholderã‚’ä½¿ã£ã¦æ¸…æ›¸ã™ã‚‹äº‹ã‚‚è€ƒãˆãŸãŒã€ãƒ–ãƒ©ã‚¦ã‚¶å´ã¨ã®åŒæœŸã«å¤±æ•—ã—ã€å‰ã®ã‚³ãƒ³ãƒ†ãƒŠã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒ
            # æ®‹ã£ã¦ã—ã¾ã„ã€ã“ã‚Œã¯ã†ã¾ãã„ã‹ãªã„ã€‚
            # streamlitã§ã¯æ—¢ã«æç”»ã—ãŸã‚‚ã®ã‚’å¤‰æ›´ã™ã‚‹ã®ã¯ã‚„ã‚ãŸæ–¹ãŒã„ã„ã‹ã‚‚ã€‚
            # ã©ã†ã—ã¦ã‚‚å†æç”»ãŒå¿…è¦ãªã‚‰(å¼•ç”¨ç•ªå·ã®æ›¸ãæ›ãˆãªã©)ã€ç´ ç›´ã«rerun()ã—ãŸæ–¹ãŒã„ã„ã€‚
#        st.rerun()
    
    except Exception as e:
        st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        st.exception(e)
        st.button("ãƒªãƒˆãƒ©ã‚¤")

# æœ€çµ‚çš„ãªå†…å®¹ã§æç”»ã—ãªãŠã™ã¹ãå ´åˆã¯ã€"need_rerun" = Trueã¨ã—ã¦ãŠã‘ã°ã“ã“ã§rerun
if st.session_state.need_rerun:
    st.session_state.need_rerun = False
    st.rerun()
